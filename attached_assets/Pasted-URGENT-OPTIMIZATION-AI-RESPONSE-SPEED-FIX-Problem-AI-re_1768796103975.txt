URGENT OPTIMIZATION: AI RESPONSE SPEED FIX

Problem: AI responses are "extremely slow"

Root Cause: Long generation times, excessive token output, unoptimized prompts

Solution: Enforce fast model with strict limits and optimized prompts

---

File to modify:

server/services/aiService.ts

CRITICAL OPTIMIZATION CHANGES:

1. ENFORCE GEMINI FLASH MODEL & TOKEN LIMITS

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

// ⚠️ CRITICAL: Use ONLY gemini-1.5-flash (FASTEST model)
const model = genAI.getGenerativeModel({ 
  model: "gemini-1.5-flash", // ⚠️ REQUIRED - DO NOT use "gemini-1.5-pro"
  
  // ⚠️ CRITICAL: Strict token limits for speed
  generationConfig: {
    maxOutputTokens: 1000,   // ⚠️ REQUIRED - Prevent long essays
    temperature: 0.7,        // Balance creativity vs speed
    topP: 0.8,
    topK: 40,
  },
});
```

2. OPTIMIZED PROMPT TEMPLATES

```typescript
// ⚠️ OPTIMIZED: Concise, fast-generating prompts

const EXPLANATION_SYSTEM_PROMPT = `You are a helpful tutor. Explain concepts CONCISELY.
RULES:
1. Get to the point immediately - NO long introductions
2. Use bullet points for speed and clarity
3. Limit to 3-5 key points maximum
4. Each bullet point: MAX 2 sentences
5. Skip examples unless absolutely necessary
6. Total response under 300 words

Format: • Key point 1 (brief)
        • Key point 2 (brief)
        • Key point 3 (brief)`;

const SUMMARIZATION_SYSTEM_PROMPT = `You are a text summarizer. Summarize CONCISELY.
RULES:
1. Extract only the MAIN ideas
2. Use bullet points for speed
3. Maximum 5 bullet points
4. Each bullet: 1 sentence maximum
5. Skip introductions and conclusions
6. Focus on actionable information only
7. Total summary under 150 words

Format: • [Main point 1]
        • [Main point 2]`;

const QUESTION_SYSTEM_PROMPT = `Generate quiz questions CONCISELY.
RULES:
1. Generate only 3 questions maximum
2. Each question: 1 sentence maximum
3. Use multiple choice format (A, B, C, D)
4. Include exactly ONE correct answer
5. Skip explanations - just questions and answers
6. Total output under 150 words

Format: 1. [Question 1]
        A) Option A
        B) Option B
        C) Option C ✓
        D) Option D`;
```

3. OPTIMIZED SERVICE FUNCTIONS

```typescript
// OPTIMIZED: Explanation service
export const generateExplanation = async (concept: string): Promise<string> => {
  try {
    // ⚠️ Concise prompt with length limits
    const prompt = `${EXPLANATION_SYSTEM_PROMPT}\n\nConcept: ${concept}\n\nKeep it BRIEF.`;
    
    const result = await model.generateContent(prompt);
    const response = await result.response.text();
    
    // ⚠️ Enforce length limit server-side
    return response.slice(0, 1000); // Hard cap at 1000 chars
  } catch (error) {
    console.error("Explanation generation error:", error);
    return "Explanation generation failed. Please try again.";
  }
};

// OPTIMIZED: Summarization service
export const generateSummary = async (text: string): Promise<string> => {
  try {
    // ⚠️ Truncate input to prevent excessive processing
    const truncatedText = text.slice(0, 2000); // Process max 2000 chars
    
    const prompt = `${SUMMARIZATION_SYSTEM_PROMPT}\n\nText: ${truncatedText}\n\nMake it SHORT.`;
    
    const result = await model.generateContent(prompt);
    const response = await result.response.text();
    
    return response.slice(0, 800); // Hard cap at 800 chars
  } catch (error) {
    console.error("Summary generation error:", error);
    return "Summary generation failed. Please try again.";
  }
};

// OPTIMIZED: Question generation service
export const generateQuestions = async (text: string): Promise<string> => {
  try {
    // ⚠️ Strict input limits for speed
    const truncatedText = text.slice(0, 1500); // Process max 1500 chars
    
    const prompt = `${QUESTION_SYSTEM_PROMPT}\n\nText: ${truncatedText}\n\nOnly 3 questions.`;
    
    const result = await model.generateContent(prompt);
    const response = await result.response.text();
    
    return response.slice(0, 600); // Hard cap at 600 chars
  } catch (error) {
    console.error("Question generation error:", error);
    return "Question generation failed. Please try again.";
  }
};

// ⚠️ NEW: Fast health check endpoint
export const checkAIHealth = async (): Promise<{ 
  status: string; 
  model: string; 
  maxTokens: number;
  responseTime?: number;
}> => {
  const startTime = Date.now();
  
  try {
    // Test with minimal prompt for speed
    const testPrompt = "Respond with only 'OK'";
    const result = await model.generateContent(testPrompt);
    const response = await result.response.text();
    
    const responseTime = Date.now() - startTime;
    
    return {
      status: response === "OK" ? "healthy" : "unexpected_response",
      model: "gemini-1.5-flash",
      maxTokens: 1000,
      responseTime
    };
  } catch (error) {
    return {
      status: "error",
      model: "gemini-1.5-flash",
      maxTokens: 1000
    };
  }
};
```

4. ADDITIONAL PERFORMANCE OPTIMIZATIONS

```typescript
// Add these optimization configurations

// ⚠️ Request timeout setting (prevent hanging)
const REQUEST_TIMEOUT = 10000; // 10 seconds maximum

// ⚠️ Retry configuration for reliability
const MAX_RETRIES = 2;
const RETRY_DELAY = 1000;

// ⚠️ Optimized generateContent with timeout and retries
const generateContentWithTimeout = async (
  prompt: string, 
  retries = MAX_RETRIES
): Promise<string> => {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), REQUEST_TIMEOUT);

  try {
    const result = await model.generateContent(prompt);
    clearTimeout(timeoutId);
    const response = await result.response.text();
    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    
    if (retries > 0) {
      console.log(`Retrying... ${retries} attempts left`);
      await new Promise(resolve => setTimeout(resolve, RETRY_DELAY));
      return generateContentWithTimeout(prompt, retries - 1);
    }
    
    throw error;
  }
};

// ⚠️ Cache frequently requested concepts
const responseCache = new Map<string, { response: string; timestamp: number }>();
const CACHE_TTL = 5 * 60 * 1000; // 5 minutes

export const getCachedOrGenerate = async (
  key: string, 
  generator: () => Promise<string>
): Promise<string> => {
  const cached = responseCache.get(key);
  
  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.response;
  }
  
  const response = await generator();
  responseCache.set(key, { response, timestamp: Date.now() });
  
  // Limit cache size
  if (responseCache.size > 100) {
    const firstKey = responseCache.keys().next().value;
    responseCache.delete(firstKey);
  }
  
  return response;
};
```

VERIFICATION CHECKLIST:

1. Model Verification:
   · Confirm model: "gemini-1.5-flash" is set
   · Confirm maxOutputTokens: 1000 is set
   · NO references to "gemini-1.5-pro"
2. Prompt Optimization:
   · All prompts include "Be concise"
   · All prompts include "Get to the point immediately"
   · All prompts enforce bullet points
   · All prompts have strict length limits
3. Performance Measures:
   · Input text truncation implemented
   · Response length capping implemented
   · Timeout protection added
   · Retry logic implemented

DEPLOYMENT COMMANDS:

```bash
# 1. Apply the fixes to aiService.ts
# 2. Restart the server
npm run build
npm start

# OR for production
npm run build
pm2 restart server

# 3. Test response times
curl http://your-server:3000/api/ai/health
```

EXPECTED RESULTS:

· Before: 10-30 second response times
· After: 2-5 second response times (70-80% faster)
· Output Length: 150-300 words instead of 800-1500 words
· Token Usage: ~50% reduction in token consumption

MONITORING METRICS TO ADD:

```typescript
// Add to your logging
console.log(`AI Response time: ${Date.now() - startTime}ms`);
console.log(`Response length: ${response.length} chars`);
```

STATUS: CRITICAL - AI performance directly impacts user experience